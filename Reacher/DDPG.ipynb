{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Goal is to solve the [reacher](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher) environment from Unity\n",
    "- A double jointed arm can move to target locations\n",
    "- A **reward of +0.1** is provided for each step, where the agent's hand is in the goal location\n",
    "- The **state space is comprised of 33 variables**; corresponding to position, rotation, velocity & angular velocities\n",
    "- **Action space is a vector of 4 numbers** having an entry b/w **+1 and -1**\n",
    "- **Solving : +30 avg. reward over 100 episodes**\n",
    "\n",
    "![Reacher.gif](reacher.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='./Reacher_Linux_NoVis/Reacher.x86')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default brain \n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Agents : 20\n",
      "State Size  : (20, 33)\n",
      "Action Size : 4\n"
     ]
    }
   ],
   "source": [
    "# Explore the env state and action space\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "# Num of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print(\"# of Agents : {}\".format(num_agents))\n",
    "# State space \n",
    "state_size = env_info.vector_observations.shape\n",
    "print(\"State Size  : {}\".format(state_size))\n",
    "# Action size\n",
    "action_size = brain.vector_action_space_size\n",
    "print(\"Action Size : {}\".format(action_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To close the environment\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# replaybuffer imports\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# Plotting imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DDPG uses stochastic behavior policy for exploration but estimates a deterministic target policy, which is easier to learn\n",
    "- Using correlated experiences leads to vast amount of variance in the approximation of the Q function\n",
    "- Thus using Experience buffer to store experiences and sample them randomly for learning in order to break the temporal correlations within epsiodes\n",
    "- Directly updating the Actor and Critic networks in DDPG causes learning algorithm to diverge\n",
    "- Using target networks for both Actor and Critic increases the stability.\n",
    "- The **TD target** and **loss function for the critic network** have equations:\n",
    "$${y_i} = {r_i} + \\gamma{Q'(s_{i+1}, \\mu'(s_{i+1}|{\\theta^{\\mu'}})|{\\theta^{Q'}})}$$\n",
    "$$L = \\frac{1}{N} \\sum_{i}(y_i - Q(s_i, a_i|\\theta^Q)^2)$$\n",
    "where,\n",
    "$$N - \\text{is the size of minibatch sampled}$$\n",
    "$$\\theta^{\\mu'} - \\text{weights of the target actor network}$$\n",
    "$$\\theta^{Q'} - \\text{weights of the target critic network}$$\n",
    "- The critic policy can be updated with the gradients from the loss function above\n",
    "- The actor network is updated with the deterministic policy gradient\n",
    "- [David Silver](http://proceedings.mlr.press/v32/silver14.pdf) proves that the gradient of the stochastic and deterministic policy is the equivalent\n",
    "$$ \\nabla_{\\theta\\mu}\\mu\\approx\\mathbb{E}_{\\mu'}[\\nabla_aQ(s, a|\\theta^Q)|_{s={s_t}, a=\\mu({s_t})} \\nabla_{\\theta^\\mu}\\mu(s|\\theta^\\mu)|_{s={s_t}}$$\n",
    "- **All this means is that we need gradients of the output of the critic network multiplied by the gradient of the output of the actor network w.r.t. it's parameters, averaged over a minibatch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models for Actor and Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, fc1=256, fc2=128, leak=0.01):\n",
    "        \"\"\" Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            hidden_size (int): Number of nodes in hidden layers\n",
    "            leak: amount of leakiness in leaky relu\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.leak = leak\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, fc1)\n",
    "        self.fc2 = nn.Linear(fc1, fc2)\n",
    "        self.fc3 = nn.Linear(fc2, action_size)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(state_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\" Initilaize the weights using He et al (2015) weights \"\"\"\n",
    "        torch.nn.init.kaiming_normal_(self.fc1.weight.data, a=self.leak, mode='fan_in')\n",
    "        torch.nn.init.kaiming_normal_(self.fc2.weight.data, a=self.leak, mode='fan_in')\n",
    "        torch.nn.init.uniform_(self.fc3.weight.data, -3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        state = self.bn(state)\n",
    "        x = F.leaky_relu(self.fc1(state), negative_slope=self.leak)\n",
    "        x = F.leaky_relu(self.fc2(x), negative_slope=self.leak)\n",
    "        x =  torch.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, fc1=256, fc2=128, fc3=128, leak=0.01):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "            hidden_size:\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.leak = leak\n",
    "        self.bn = nn.BatchNorm1d(state_size)\n",
    "        self.fcs1 = nn.Linear(state_size, fc1)\n",
    "        self.fc2 = nn.Linear(fc1 + action_size, fc2)\n",
    "        self.fc3 = nn.Linear(fc2, fc3)\n",
    "        self.fc4 = nn.Linear(fc3, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\" Initilaize the weights using He et al (2015) weights \"\"\"\n",
    "        torch.nn.init.kaiming_normal_(self.fcs1.weight.data, a=self.leak, mode='fan_in')\n",
    "        torch.nn.init.kaiming_normal_(self.fc2.weight.data, a=self.leak, mode='fan_in')\n",
    "        torch.nn.init.uniform_(self.fc3.weight.data, -3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\" Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        state = self.bn(state)\n",
    "        x = F.leaky_relu(self.fcs1(state), negative_slope=self.leak)\n",
    "        x = torch.cat((x, action), dim=1)\n",
    "        x = F.leaky_relu(self.fc2(x), negative_slope=self.leak)\n",
    "        x = F.leaky_relu(self.fc3(x), negative_slope=self.leak)\n",
    "        x =  self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer\n",
    "\n",
    "- The replay buffer is used to solve the problem of correlation bw episodes used for learning since we randomly sample experiences to learn from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ornstein-Uhlenbeck Process\n",
    "- Since the policy is deterministic, it can produce the same actions\n",
    "- For exploration we add additive noise to deterministic action to explore the action\n",
    "- To generate the correlated noise we use OU process given by equation\n",
    "$$ dx_t = \\theta(\\mu - x_t) + \\sigma dW_t$$  \n",
    "$$ \\text{where,  } dW_t = \\mathcal{N}(0, dt)$$\n",
    "$$ \\mu = \\text{mean of the process}$$\n",
    "$$ \\theta = \\text{friction, how fast it varies with noise} $$\n",
    "$$ \\sigma = \\text{controls the amount of random noise} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([np.random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 1024       # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor\n",
    "LR_CRITIC = 3e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0.0001   # L2 weight decay\n",
    "LEAKINESS = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, n_agents=1):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            n_agents: number of agents it will control in the environment\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.n_agents = n_agents\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, leak=LEAKINESS).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, leak=LEAKINESS).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, leak=LEAKINESS).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, leak=LEAKINESS).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC)\n",
    "        # self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE)\n",
    "        self.timesteps = 0  \n",
    "\n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\" Given a batch of S,A,R,S' experiences, it saves them into the\n",
    "            experience buffer, and occasionally samples from the experience\n",
    "            buffer to perform training steps.\n",
    "        \"\"\"\n",
    "        self.timesteps += 1\n",
    "        for i in range(self.n_agents):\n",
    "            self.memory.add(states[i], actions[i], rewards[i], next_states[i], dones[i])\n",
    "\n",
    "        if (len(self.memory) > BATCH_SIZE) and (self.timesteps % 20 == 0):\n",
    "            for _ in range(10):\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    \n",
    "    def act(self, states, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_local(states).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            actions = actions + [self.noise.sample() for _ in range(self.n_agents)]\n",
    "        return np.clip(actions, -1, 1)\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "\n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 33\n",
    "action_size = 4\n",
    "num_agents = 20\n",
    "\n",
    "# Create agent\n",
    "agent = Agent(state_size=state_size, action_size=action_size, n_agents=num_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=2000, max_t=1000, window_size=100, score_threshold=30.0, \n",
    "         print_interval=10, episodes=1000):\n",
    "\n",
    "    scores_deque = deque(maxlen=window_size) \n",
    "    scores = []        \n",
    "    best_average_score = 0\n",
    "    \n",
    "    for i_episode in range(1, episodes+1):\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        \n",
    "        agent.reset()\n",
    "        episode_scores = np.zeros(num_agents) \n",
    "\n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            #print(f\"Reward : {rewards}\")\n",
    "            agent.step(states=states, actions=actions, rewards=rewards, next_states=next_states, dones=dones)\n",
    "            episode_scores += np.array(rewards)\n",
    "            states = next_states\n",
    "            if np.any(dones):\n",
    "                break\n",
    "\n",
    "        episode_score = np.mean(episode_scores)\n",
    "        scores_deque.append(episode_score)\n",
    "        scores.append(episode_score)\n",
    "        average_score = np.mean(scores_deque)\n",
    "\n",
    "        print('\\rEpisode: {}\\tAverage Score: {:.2f}\\tCurrent Score: {:.2f}'.format(i_episode, average_score, episode_score), end=\"\")\n",
    "        if i_episode % print_interval == 0:\n",
    "            print('\\rEpisode: {}\\tAverage Score: {:.2f}\\tCurrent Score: {:.2f}'.format(i_episode, average_score, episode_score))\n",
    "\n",
    "        if average_score >= score_threshold:\n",
    "            print('\\nEnvironment solved in {} episodes!\\tAverage Score: {:.2f}'.format(i_episode-window_size, average_score))\n",
    "            torch.save(agent.actor_local.state_dict(), 'actor.ckpt')\n",
    "            torch.save(agent.critic_local.state_dict(), 'critic.ckpt')\n",
    "            break\n",
    "\n",
    "    np.save('scores.npy', scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10\tAverage Score: 0.51\tCurrent Score: 0.95\n",
      "Episode: 20\tAverage Score: 0.75\tCurrent Score: 1.05\n",
      "Episode: 30\tAverage Score: 0.98\tCurrent Score: 1.88\n",
      "Episode: 40\tAverage Score: 1.40\tCurrent Score: 2.94\n",
      "Episode: 50\tAverage Score: 1.87\tCurrent Score: 4.35\n",
      "Episode: 60\tAverage Score: 2.39\tCurrent Score: 5.00\n",
      "Episode: 70\tAverage Score: 2.77\tCurrent Score: 5.22\n",
      "Episode: 80\tAverage Score: 3.23\tCurrent Score: 6.51\n",
      "Episode: 90\tAverage Score: 3.71\tCurrent Score: 9.58\n",
      "Episode: 100\tAverage Score: 4.29\tCurrent Score: 10.12\n",
      "Episode: 110\tAverage Score: 5.14\tCurrent Score: 9.00\n",
      "Episode: 120\tAverage Score: 6.10\tCurrent Score: 12.28\n",
      "Episode: 130\tAverage Score: 7.28\tCurrent Score: 14.71\n",
      "Episode: 140\tAverage Score: 8.66\tCurrent Score: 18.87\n",
      "Episode: 150\tAverage Score: 10.37\tCurrent Score: 20.98\n",
      "Episode: 160\tAverage Score: 12.11\tCurrent Score: 21.47\n",
      "Episode: 170\tAverage Score: 13.95\tCurrent Score: 26.43\n",
      "Episode: 180\tAverage Score: 15.86\tCurrent Score: 25.15\n",
      "Episode: 190\tAverage Score: 17.68\tCurrent Score: 26.00\n",
      "Episode: 200\tAverage Score: 19.28\tCurrent Score: 23.58\n",
      "Episode: 210\tAverage Score: 20.94\tCurrent Score: 24.58\n",
      "Episode: 220\tAverage Score: 22.35\tCurrent Score: 26.76\n",
      "Episode: 230\tAverage Score: 23.67\tCurrent Score: 27.40\n",
      "Episode: 240\tAverage Score: 24.71\tCurrent Score: 26.99\n",
      "Episode: 250\tAverage Score: 25.42\tCurrent Score: 29.07\n",
      "Episode: 260\tAverage Score: 26.26\tCurrent Score: 32.09\n",
      "Episode: 270\tAverage Score: 27.10\tCurrent Score: 31.20\n",
      "Episode: 280\tAverage Score: 27.59\tCurrent Score: 30.32\n",
      "Episode: 290\tAverage Score: 27.85\tCurrent Score: 26.14\n",
      "Episode: 300\tAverage Score: 28.11\tCurrent Score: 28.80\n",
      "Episode: 310\tAverage Score: 28.33\tCurrent Score: 28.67\n",
      "Episode: 320\tAverage Score: 28.81\tCurrent Score: 30.51\n",
      "Episode: 330\tAverage Score: 29.10\tCurrent Score: 29.76\n",
      "Episode: 340\tAverage Score: 29.39\tCurrent Score: 30.77\n",
      "Episode: 350\tAverage Score: 29.33\tCurrent Score: 26.31\n",
      "Episode: 360\tAverage Score: 28.86\tCurrent Score: 27.26\n",
      "Episode: 370\tAverage Score: 28.37\tCurrent Score: 28.95\n",
      "Episode: 380\tAverage Score: 27.99\tCurrent Score: 25.90\n",
      "Episode: 390\tAverage Score: 28.01\tCurrent Score: 27.14\n",
      "Episode: 400\tAverage Score: 28.05\tCurrent Score: 30.17\n",
      "Episode: 410\tAverage Score: 28.11\tCurrent Score: 29.04\n",
      "Episode: 420\tAverage Score: 27.90\tCurrent Score: 26.88\n",
      "Episode: 430\tAverage Score: 27.90\tCurrent Score: 30.87\n",
      "Episode: 440\tAverage Score: 27.76\tCurrent Score: 26.84\n",
      "Episode: 450\tAverage Score: 27.92\tCurrent Score: 31.40\n",
      "Episode: 460\tAverage Score: 28.33\tCurrent Score: 31.11\n",
      "Episode: 470\tAverage Score: 28.69\tCurrent Score: 29.36\n",
      "Episode: 480\tAverage Score: 29.11\tCurrent Score: 31.60\n",
      "Episode: 490\tAverage Score: 29.42\tCurrent Score: 31.92\n",
      "Episode: 500\tAverage Score: 29.75\tCurrent Score: 33.93\n",
      "Episode: 506\tAverage Score: 30.05\tCurrent Score: 33.43\n",
      "Environment solved in 406 episodes!\tAverage Score: 30.05\n"
     ]
    }
   ],
   "source": [
    "scores = ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure()\n",
    "ax = f.add_subplot(111)\n",
    "plt.plot(range(1, len(scores) + 1), scores)\n",
    "plt.xlabel('# Episodes')\n",
    "plt.ylabel('Scores')\n",
    "plt.savefig('scores_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "[Patrick Emami - Deep Deterministic Policy Gradients in TensorFlow](http://proceedings.mlr.press/v32/silver14.pdf)   \n",
    "[David Silver - Deep deterministic Policy Gradients (**Theory**)](http://proceedings.mlr.press/v32/silver14.pdf)   \n",
    "[Julian Vitey - Deep RL (**DDPG algorithm**)](https://julien-vitay.net/deeprl/DeepRL.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
