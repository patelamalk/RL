{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Goal is to solve the [reacher](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher) environment from Unity\n",
    "- A double jointed arm can move to target locations\n",
    "- A **reward of +0.1** is provided for each step, where the agent's hand is in the goal location\n",
    "- The **state space is comprised of 33 variables**; corresponding to position, rotation, velocity & angular velocities\n",
    "- **Action space is a vector of 4 numbers** having an entry b/w **+1 and -1**\n",
    "- **Solving : +30 avg. reward over 100 episodes**\n",
    "\n",
    "![Alt text](./reacher.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# Import the no visual env\n",
    "#env = UnityEnvironment(file_name=\"../Reacher_Linux_NoVis/Reacher.x86_64\")\n",
    "env = UnityEnvironment(file_name='Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default brain \n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Agents : 20\n",
      "State Size  : (20, 33)\n",
      "Action Size : 4\n"
     ]
    }
   ],
   "source": [
    "# Explore the env state and action space\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "# Num of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print(\"# of Agents : {}\".format(num_agents))\n",
    "# State space \n",
    "state_size = env_info.vector_observations.shape\n",
    "print(\"State Size  : {}\".format(state_size))\n",
    "# Action size\n",
    "action_size = brain.vector_action_space_size\n",
    "print(\"Action Size : {}\".format(action_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To close the environment\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-86d9c5c545bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Torch imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# replaybuffer imports\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# Plotting imports\"\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#from workspace_utils import active_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DDPG uses stochastic behavior policy for exploration but estimates a deterministic target policy, which is easier to learn\n",
    "- Using correlated experiences leads to vast amount of variance in the approximation of the Q function\n",
    "- Thus using Experience buffer to store experiences and sample them randomly for learning in order to break the temporal correlations within epsiodes\n",
    "- Directly updating the Actor and Critic networks in DDPG causes learning algorithm to diverge\n",
    "- Using target networks for both Actor and Critic increases the stability.\n",
    "- The **TD target** and **loss function for the critic network** have equations:\n",
    "$${y_i} = {r_i} + \\gamma{Q'(s_{i+1}, \\mu'(s_{i+1}|{\\theta^{\\mu'}})|{\\theta^{Q'}})}$$\n",
    "$$L = \\frac{1}{N} \\sum_{i}(y_i - Q(s_i, a_i|\\theta^Q)^2)$$\n",
    "where,\n",
    "$$N - \\text{is the size of minibatch sampled}$$\n",
    "$$\\theta^{\\mu'} - \\text{weights of the target actor network}$$\n",
    "$$\\theta^{Q'} - \\text{weights of the target critic network}$$\n",
    "- The critic policy can be updated with the gradients from the loss function above\n",
    "- The actor network is updated with the deterministic policy gradient\n",
    "- [David Silver](http://proceedings.mlr.press/v32/silver14.pdf) proves that the gradient of the stochastic and deterministic policy is the equivalent\n",
    "$$ \\nabla_{\\theta\\mu}\\mu\\approx\\mathbb{E}_{\\mu'}[\\nabla_aQ(s, a|\\theta^Q)|_{s={s_t}, a=\\mu({s_t})} \\nabla_{\\theta^\\mu}\\mu(s|\\theta^\\mu)|_{s={s_t}}$$\n",
    "- **All this means is that we need gradients of the output of the critic network multiplied by the gradient of the output of the actor network w.r.t. it's parameters, averaged over a minibatch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models for Actor and Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, fc1_units=256, fc2_units=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): #1 layer nodes\n",
    "            fc2_units (int): #2 layer nodes\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.bn1 = nn.BatchNorm1d(fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.bn2 = nn.BatchNorm1d(fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.bn1(F.relu(self.fc1(state)))\n",
    "        x = self.bn2(F.relu(self.fc2(x)))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, fcs1_units=256, fc2_units=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            fc1_units (int): #1 layer nodes\n",
    "            fc2_units (int): #2 layer nodes\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.bn1 = nn.BatchNorm1d(fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units + action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = self.bn1(F.leaky_relu(self.fcs1(state)))\n",
    "        x = torch.cat((x, action), dim=1)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer\n",
    "\n",
    "- The replay buffer is used to solve the problem of correlation bw episodes used for learning since we randomly sample experiences to learn from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ornstein-Uhlenbeck Process\n",
    "- Since the policy is deterministic, it can produce the same actions\n",
    "- For exploration we add additive noise to deterministic action to explore the action\n",
    "- To generate the correlated noise we use OU process given by equation\n",
    "$$ dx_t = \\theta(\\mu - x_t) + \\sigma dW_t$$  \n",
    "$$ \\text{where,  } dW_t = \\mathcal{N}(0, dt)$$\n",
    "$$ \\mu = \\text{mean of the process}$$\n",
    "$$ \\theta = \\text{friction, how fast it varies with noise} $$\n",
    "$$ \\sigma = \\text{controls the amount of random noise} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([np.random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 3e-4         # learning rate of the actor\n",
    "LR_CRITIC = 3e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, n_agents=1):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            n_agents: number of agents\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.n_agents = n_agents\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC)\n",
    "        # self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE)\n",
    "        self.timesteps = 0  \n",
    "\n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\" Saves a batch of S, A, R, S', D \n",
    "            Learns from the experiences periodically\n",
    "        \"\"\"\n",
    "        self.timesteps += 1\n",
    "        for i in range(self.n_agents):\n",
    "            self.memory.add(states[i], actions[i], rewards[i], next_states[i], dones[i])\n",
    "\n",
    "        if (len(self.memory) > BATCH_SIZE) and (self.timesteps % 20 == 0):\n",
    "            for _ in range(10):\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    \n",
    "    def act(self, states, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_local(states).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            actions += [self.noise.sample() for _ in range(self.n_agents)]\n",
    "        return np.clip(actions, -1, 1)\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "\n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 33\n",
    "action_size = 4\n",
    "\n",
    "# Create agent\n",
    "agent = Agent(state_size=state_size, action_size=action_size, n_agents=num_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=2_000, max_t=1_000, window_size=100, score_threshold=30.0, \n",
    "         print_interval=10, episodes=1000):\n",
    "\n",
    "    scores_deque = deque(maxlen=window_size) \n",
    "    scores = []        \n",
    "    best_average_score = 0\n",
    "    \n",
    "    for i_episode in range(1, episodes+1):\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        \n",
    "        agent.reset()\n",
    "        episode_scores = np.zeros(num_agents) \n",
    "\n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            #print(f\"Reward : {rewards}\")\n",
    "            agent.step(states=states, actions=actions, rewards=rewards, next_states=next_states, dones=dones)\n",
    "            episode_scores += np.array(rewards)\n",
    "            states = next_states\n",
    "            if np.any(dones):\n",
    "                break\n",
    "\n",
    "        episode_score = np.mean(episode_scores)\n",
    "        scores_deque.append(episode_score)\n",
    "        scores.append(episode_score)\n",
    "        average_score = np.mean(scores_deque)\n",
    "\n",
    "        print('\\rEpisode: {}\\tAverage Score: {:.2f}\\tCurrent Score: {:.2f}'.format(i_episode, average_score, episode_score), end=\"\")\n",
    "        if i_episode % print_interval == 0:\n",
    "            print('\\rEpisode: {}\\tAverage Score: {:.2f}\\tCurrent Score: {:.2f}'.format(i_episode, average_score, episode_score))\n",
    "\n",
    "        if average_score >= score_threshold:\n",
    "            print('\\nEnvironment solved in {} episodes!\\tAverage Score: {:.2f}'.format(i_episode-window_size, average_score))\n",
    "            torch.save(agent.actor_local.state_dict(), 'actor.ckpt')\n",
    "            torch.save(agent.critic_local.state_dict(), 'critic.ckpt')\n",
    "            break\n",
    "\n",
    "    np.save('scores.npy', scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10\tAverage Score: 0.65\tCurrent Score: 1.15\n",
      "Episode: 20\tAverage Score: 1.11\tCurrent Score: 1.88\n",
      "Episode: 30\tAverage Score: 1.69\tCurrent Score: 3.29\n",
      "Episode: 40\tAverage Score: 2.29\tCurrent Score: 4.23\n",
      "Episode: 50\tAverage Score: 2.88\tCurrent Score: 6.01\n",
      "Episode: 60\tAverage Score: 3.66\tCurrent Score: 10.09\n",
      "Episode: 70\tAverage Score: 4.62\tCurrent Score: 11.45\n",
      "Episode: 80\tAverage Score: 5.73\tCurrent Score: 15.08\n",
      "Episode: 90\tAverage Score: 6.86\tCurrent Score: 15.88\n",
      "Episode: 100\tAverage Score: 8.36\tCurrent Score: 29.27\n",
      "Episode: 110\tAverage Score: 11.46\tCurrent Score: 33.75\n",
      "Episode: 120\tAverage Score: 14.59\tCurrent Score: 32.83\n",
      "Episode: 130\tAverage Score: 17.49\tCurrent Score: 33.47\n",
      "Episode: 140\tAverage Score: 20.54\tCurrent Score: 34.42\n",
      "Episode: 150\tAverage Score: 23.63\tCurrent Score: 36.89\n",
      "Episode: 160\tAverage Score: 26.43\tCurrent Score: 34.72\n",
      "Episode: 170\tAverage Score: 28.68\tCurrent Score: 33.93\n",
      "Episode: 177\tAverage Score: 30.04\tCurrent Score: 31.59\n",
      "Environment solved in 77 episodes!\tAverage Score: 30.04\n"
     ]
    }
   ],
   "source": [
    "#with active_session():\n",
    "scores = ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure()\n",
    "ax = f.add_subplot(111)\n",
    "plt.plot(range(1, len(scores) + 1), scores)\n",
    "plt.xlabel('# Episodes')\n",
    "plt.ylabel('Scores')\n",
    "plt.savefig('scores_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "[Patrick Emami - Deep Deterministic Policy Gradients in TensorFlow](http://proceedings.mlr.press/v32/silver14.pdf)   \n",
    "[David Silver - Deep deterministic Policy Gradients (**Theory**)](http://proceedings.mlr.press/v32/silver14.pdf)   \n",
    "[Julian Vitey - Deep RL (**DDPG algorithm**)](https://julien-vitay.net/deeprl/DeepRL.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
