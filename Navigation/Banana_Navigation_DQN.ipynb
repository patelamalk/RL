{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the classes\n",
    "from Agent import Agent\n",
    "from model import DQN\n",
    "from ReplayBuffer import ReplayBuffer\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "env = UntiyEnvironment(filename=\"/data/Banana_Linux_NoVis/Banana.x86_64\")\n",
    "\n",
    "# Check the brain name\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space\n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Explore the state space and actions\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BUFFER_SIZE = int(1e5)   # replay buffer size\n",
    "BATCH_SIZE = 64          # minibatch size\n",
    "GAMMA = 0.99             # discounting the rewards\n",
    "LR = 5e-3                # learning rate for the agent\n",
    "UPDATE_EVERY = 5         # how often to update the network\n",
    "TAU = 1e-3               # for soft update of target parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Train loop\n",
    "\n",
    "def train_agent(n_episodes, max_t, epsilon_start, epsilon_end, epsilon_decay):\n",
    "    # Initialize the agent\n",
    "    agent = Agent(state_size=37, action_size=4, device=device, buffer_size=BUFFER_SIZE, gamma=GAMMA, tau=TAU, lr=LR, update_every=UPDATE_EVERY, fc1=128, fc2=256, fc3=64)\n",
    "    # Initilalize the scores list\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    epsilon = epsilon_start\n",
    "    # Start the episodes\n",
    "    for episode in range(1, n_episodes):\n",
    "        # Reset the environment\n",
    "        state = env.reset(train_mode=True)[brain_name].vector_observations[0]\n",
    "        score = 0\n",
    "        # start the time steps in an episode\n",
    "        for t in range(max_t):\n",
    "            # select the action\n",
    "            action = agent.act(state, epsilon)\n",
    "            # Get the next state, reward, done\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            # Agent takes a step, adds the experience and learns\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        epsilon = max(epsilon_end, epsilon_decay*epsilon)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_window)), end=\"\")\n",
    "        if episode%100==0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=13.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "\n",
    "        # Save the model weights\n",
    "        torch.save(agent.local_nw.state_dict(), \"weights.pt\")\n",
    "    return scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_scores = train_agent(1_800, 1_000, 1.0, 0.01, 0.995)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}