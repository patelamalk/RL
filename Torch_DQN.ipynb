{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Torch DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Qym5sFmXmruv"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qym5sFmXmruv",
        "colab_type": "text"
      },
      "source": [
        "### pytorch basics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf-LIRoxmxTc",
        "colab_type": "code",
        "outputId": "f2a6f29d-9e12-4bf0-fa0a-e2d32b1f3cfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "torch.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.5.0+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAqaw8uWmzYs",
        "colab_type": "code",
        "outputId": "a7067162-40c7-4b66-b3fa-7606249c37f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "l = nn.Linear(2, 5).cuda()\n",
        "v = torch.FloatTensor([1, 2]).cuda()\n",
        "l(v)\n",
        "# Feed forward layer with 2 inputs and 5 outputs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.5842, -0.4360, -0.0710, -0.3440,  0.1779], device='cuda:0',\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igjXeSTjnLZ3",
        "colab_type": "code",
        "outputId": "0e1c64e2-18cd-495b-fd05-f8928e0d80db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Create a sequential network\n",
        "# 3 layer\n",
        "# Applied along dimension 1\n",
        "s = nn.Sequential(\n",
        "      nn.Linear(2, 5),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(5, 2),\n",
        "      nn.Dropout(0.3),\n",
        "      nn.Softmax(dim=1)\n",
        ").cuda()\n",
        "s"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=2, out_features=5, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=5, out_features=2, bias=True)\n",
              "  (3): Dropout(p=0.3, inplace=False)\n",
              "  (4): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBdC2nu5p58T",
        "colab_type": "code",
        "outputId": "a2db3cd5-5e71-430a-c6b7-47ce0d835d96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "s(torch.FloatTensor([[1, 2], [2, 3]]).cuda())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1891, 0.8109],\n",
              "        [0.1830, 0.8170]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2APOtNlxp6UP",
        "colab_type": "code",
        "outputId": "14e95dbc-0526-40b6-a996-d8749a02db7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Custom layers\n",
        "class CustomModule(nn.Module):\n",
        "    def __init__(self, num_input, num_classes, dropout_prob=0.3):\n",
        "        super(CustomModule, self).__init__()\n",
        "        self.pipe = nn.Sequential(\n",
        "            nn.Linear(num_input, 5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(5, 20),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(20, num_classes),\n",
        "            nn.Dropout(p=dropout_prob),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.pipe(x)\n",
        "\n",
        "net = CustomModule(2, 3).cuda()\n",
        "v = torch.FloatTensor([[2, 3]]).cuda()\n",
        "out = net(v)\n",
        "print(net)\n",
        "print(out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CustomModule(\n",
            "  (pipe): Sequential(\n",
            "    (0): Linear(in_features=2, out_features=5, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=5, out_features=20, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
            "    (5): Dropout(p=0.3, inplace=False)\n",
            "    (6): Softmax(dim=1)\n",
            "  )\n",
            ")\n",
            "tensor([[0.3441, 0.3813, 0.2746]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jSK6tmxAEeb",
        "colab_type": "text"
      },
      "source": [
        "**Normal Training loop:**\n",
        "  \n",
        "for x, y in iter(batches):            \n",
        "> x = torch.tensor(x)    \n",
        "  y = torch.tensor(y)          \n",
        "  out = network(x)      \n",
        "  loss = loss_function(out, y)      \n",
        "  loss.backward()        \n",
        "  optimizer.step()      \n",
        "  optimizer.zero_grad()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUZPzuEODBMb",
        "colab_type": "text"
      },
      "source": [
        "### Deep Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-420Tc-LDFCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import collections\n",
        "import numpy as np\n",
        "import time\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNwmIYltQTrd",
        "colab_type": "text"
      },
      "source": [
        "**DQN training**\n",
        "\n",
        "**Algorithm Source : https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf**\n",
        "\n",
        "1.   Initialize the parameters $Q(s, a)$ and $\\hat{Q}(s, a)$ with random weights, $\\epsilon$=1.0 and empty replay buffer.\n",
        "2.   with probability $\\epsilon$ select a random action, otherwise $a = argmax Q(s, a)$\n",
        "3.   Execute action a in the environment and observe reward, state $s'$\n",
        "4.   Store transition $(s, a, r, s')$ in replay buffer.\n",
        "5.   Sample a random min-batch of transitions from replay buffer.\n",
        "6.   Calculate the target $y = r$ and if ended $y = r + \\gamma max Q(s',a')$\n",
        "7.   Loss = $(Q(s, a) - y)^2$\n",
        "8.   Update the $Q(s, a)$ using SGD algorithm, minimizing loss. \n",
        "9.   Every N steps, copy weights from $Q$ to $\\hat{Q}$\n",
        "10.  Repeat from step 2. until converged.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbCdc85IEkmX",
        "colab_type": "text"
      },
      "source": [
        "#### Pre Process frames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViHuABXnEnyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To convert to 84x84 from the original paper \n",
        "# Implementing using the link below, torchvision.transform\n",
        "# https://pytorch.org/docs/stable/torchvision/transforms.html\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4itZM5qKaHo",
        "colab_type": "text"
      },
      "source": [
        "#### Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqKsMQq0Rm0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convolutional network\n",
        "# pyTorch doesn't have a flatten layer, use a helper function conv_out\n",
        "# This dummy call happens once for model creation\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        out_size = self.get_conv_out(input_shape)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "    \n",
        "    def get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the result tensor, this doesnot create a new memory space hance no overhead\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)  \n",
        "        return self.fc(conv_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkEltGOkKlKk",
        "colab_type": "text"
      },
      "source": [
        "#### Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_6FpQNYswDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "gamma = 0.99                              # Gamma for the bellman equation\n",
        "batch_size = 32                           # Batch sampled from replay buffer\n",
        "replay_size = 10_000                      # Max capacity of the buffer\n",
        "replay_start_size = 10_000                # No. of frames before repopulate\n",
        "learning_rate = 1e-4                      # Learning rate for the optimizer\n",
        "sync_target = 1_000                       # How frequently to update the target model\n",
        "epsilon_start = 1.0             \n",
        "epsilon_final = 0.01\n",
        "epsilon_decay_last_frame = 300_000        # After 300_000 frames the epsilon is decayed to 0.1, random exploration"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtbAp9XMxurq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Experience Buffer\n",
        "Experience = collections.namedtuple(\n",
        "    'Experience', field_names = ['state', 'action', 'reward', 'done', 'new_state']\n",
        ")\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "        return np.array(states), np.array(actions), np.array(rewards), np.array(dones), np.array(next_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBLF4jZlCJN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Agent class\n",
        "class Agent:\n",
        "    def __init__(self, env, exp_buffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self._reset()\n",
        "    def _reset():\n",
        "        self.state = env.reset()\n",
        "        self.total_reward = 0.0\n",
        "    def step(self, net, epsilon):\n",
        "        done_reward = 0\n",
        "        # Greedy action\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            state_a = np.array([self.state])\n",
        "            state_v = torch.tensor(state_a).cuda()\n",
        "            q_val = net(state_v)\n",
        "            _, act_v = torch.max(q_val, dim=1)\n",
        "            action = int(act_v.item())\n",
        "        # After the action is chosen add it to the replay buffer\n",
        "            new_state, reward, is_done, _ = self.env.step()\n",
        "            self.total_reward += reward\n",
        "            exp = Experience(self.state, action, reward, is_done, new_state)\n",
        "            self.exp_buffer.append(exp)\n",
        "            self.state = new_state\n",
        "            if is_done:\n",
        "                done_reward = self.total_reward\n",
        "                self._reset()\n",
        "            return done_reward      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNLre2G8DAod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss Calculation\n",
        "def calc_loss(batch, net, target):\n",
        "      states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "      states_v = torch.tensor(np.array(states)).cuda()\n",
        "      next_states_v = torch.tensor(np.array(next_states)).cuda()\n",
        "      actions_v = torch.tensor(actions).cuda()\n",
        "      rewards_v = torch.tensor(rewards).cuda()\n",
        "      done_mask = torch.BoolTensor(dones).cuda()\n",
        "\n",
        "      state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "      next_state_values = target(next_states_v).max(1)[0]\n",
        "\n",
        "      next_state_values[done_mask] = 0\n",
        "      # Prevent gradients from flowing into target n/w\n",
        "      next_state_values = next_state_values.detach()\n",
        "      # Bellman approximation\n",
        "      expected_state_action_values = next_state_values * gamma + rewards_v\n",
        "\n",
        "      return nn.MSELoss()(state_action_values, expected_state_action_values)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBjToexFDf67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dummy loop [Change this to fit the actual problem]\n",
        "\n",
        "\"\"\"\n",
        "Load the env\n",
        "print observation space\n",
        "print the action space\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# main network\n",
        "net = DQN(env.observation_space.shape, env.action_space.shape).cuda()\n",
        "# Target network\n",
        "target = DQN(env.observation_space.shape, env.action_space.shape).cuda()\n",
        "# Replay buffer\n",
        "buffer = ExperienceBuffer(replay_size)\n",
        "# agent\n",
        "agent = Agent(env, buffer)\n",
        "# epsilon \n",
        "epsilon = epsilon_start\n",
        "# optimizer\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "# Rewards\n",
        "total_rewards = []\n",
        "# index used for reducing the epsilon\n",
        "\n",
        "while True:\n",
        "    epsilon = max(epsilon_final, epsilon_start - index / epsilon_decay_last_frame)\n",
        "    reward = agent.step(net, epsilon)\n",
        "    if done:\n",
        "        break\n",
        "    else:\n",
        "        total_rewards.append(reward)\n",
        "        if len(buffer) < replay_start_size:\n",
        "            continue\n",
        "        if index % sync_target == 0:\n",
        "            # Load the weights of the main network on the target network\n",
        "            target.load_state_dict(net.state_dict())\n",
        "        optimizer.zero_grad()\n",
        "        batch = buffer.sample(batch_size)\n",
        "        loss_t = calc_loss(batch, net, target)\n",
        "        loss_t.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUWqTF5hMv2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}